package main

import (
	"context"
	"flag"
	"fmt"
	"io/ioutil"
	"os"
	"regexp"
	"strings"
	"time"

	"github.com/CRASH-Tech/proxmox-operator/cmd/common"
	kuberentes "github.com/CRASH-Tech/proxmox-operator/cmd/kubernetes"
	"github.com/CRASH-Tech/proxmox-operator/cmd/kubernetes/api/v1alpha1"
	"github.com/CRASH-Tech/proxmox-operator/cmd/proxmox"
	log "github.com/sirupsen/logrus"
	"gopkg.in/yaml.v2"
	"k8s.io/client-go/dynamic"
	"k8s.io/client-go/rest"
	"k8s.io/client-go/tools/clientcmd"
)

var (
	version = "0.0.1"
	config  common.Config
)

func init() {
	var configPath string
	flag.StringVar(&configPath, "c", "config.yaml", "config file path. Default: config.yaml")
	c, err := readConfig(configPath)
	if err != nil {
		log.Fatal(err)
	}
	config = c

	switch config.Log.Format {
	case "text":
		log.SetFormatter(&log.TextFormatter{})
	case "json":
		log.SetFormatter(&log.JSONFormatter{})
	default:
		log.SetFormatter(&log.TextFormatter{})
	}

	switch config.Log.Level {
	case "info":
		log.SetLevel(log.InfoLevel)
	case "warn":
		log.SetLevel(log.WarnLevel)
	case "debug":
		log.SetLevel(log.DebugLevel)
	default:
		log.SetLevel(log.InfoLevel)
	}

	var restConfig *rest.Config
	if path, isSet := os.LookupEnv("KUBECONFIG"); isSet {
		log.Printf("using configuration from '%s'", path)
		restConfig, err = clientcmd.BuildConfigFromFlags("", path)
		if err != nil {
			log.Fatal(err)
		}
	} else {
		log.Printf("using in-cluster configuration")
		restConfig, err = rest.InClusterConfig()
		if err != nil {
			log.Fatal(err)
		}
	}
	config.DynamicClient = dynamic.NewForConfigOrDie(restConfig)
}

func main() {
	log.Infof("Starting proxmox-operator %s\n", version)

	ctx := context.Background()
	kClient := kuberentes.NewClient(ctx, *config.DynamicClient)
	pClient := proxmox.NewClient(config.Clusters)

	for {
		processV1aplha1(kClient, pClient)

		time.Sleep(5 * time.Second)
	}
}

func readConfig(path string) (common.Config, error) {
	config := common.Config{}
	config.Clusters = make(map[string]proxmox.ClusterApiConfig)

	yamlFile, err := ioutil.ReadFile(path)
	if err != nil {
		return common.Config{}, err
	}
	err = yaml.Unmarshal(yamlFile, &config)
	if err != nil {
		return common.Config{}, err
	}

	return config, err
}



func deleteQemu(kClient *kuberentes.Client, pClient *proxmox.Client, qemu v1alpha1.Qemu) error {
	if qemu.Status.Deploy == v1alpha1.STATUS_DEPLOY_EMPTY {
		qemu.RemoveFinalizers()
		_, err := kClient.V1alpha1().Qemu().Patch(qemu)
		if err != nil {
			return fmt.Errorf("cannot remove qemu finalizer: %s", err)
		}
		return nil
	}

	qemu.Status.Deploy = v1alpha1.STATUS_DEPLOY_DELETING
	qemu, err := kClient.V1alpha1().Qemu().UpdateStatus(qemu)
	if err != nil {
		return fmt.Errorf("cannot update qemu status: %s", err)
	}

	if !isQemuPlaced(qemu) {
		return nil
	}

	if qemu.Spec.Autostop {
		err = pClient.Cluster(qemu.Status.Cluster).Node(qemu.Status.Node).Qemu().Stop(qemu.Status.VmId)
		if err != nil {
			return fmt.Errorf("cannot stop: %s", err)
		}
	}

	qemu, err = syncQemuPowerStatus(kClient, pClient, qemu)
	if err != nil {
		return fmt.Errorf("cannot get qemu power status: %s", err)
	}
	if qemu.Status.Power == v1alpha1.STATUS_POWER_ON {
		log.Warnf("Waiting qemu stop for deletion: %s", qemu.Metadata.Name)
	} else {
		err = pClient.Cluster(qemu.Status.Cluster).Node(qemu.Status.Node).Qemu().Delete(qemu.Status.VmId)
		if err != nil {
			return fmt.Errorf("cannot delete qemu: %s", err)
		}

		qemu.RemoveFinalizers()
		_, err = kClient.V1alpha1().Qemu().Patch(qemu)
		if err != nil {
			return fmt.Errorf("cannot remove qemu finalizer: %s", err)
		}
	}

	return nil
}

func syncQemuDeployStatus(kClient *kuberentes.Client, pClient *proxmox.Client, qemu v1alpha1.Qemu) (v1alpha1.Qemu, error) {
	if !isQemuPlaced(qemu) {
		return qemu, nil
	}

	pendingConfig, err := pClient.Cluster(qemu.Status.Cluster).Node(qemu.Status.Node).Qemu().GetPendingConfig(qemu.Status.VmId)
	if err != nil {
		qemu.Status.Deploy = v1alpha1.STATUS_DEPLOY_ERROR
		qemu, err = kClient.V1alpha1().Qemu().UpdateStatus(qemu)
		if err != nil {
			return qemu, err
		}
		return qemu, fmt.Errorf("cannot get pending config: %s", err)
	}

	var isPending bool
	for _, v := range pendingConfig {
		if v.Pending != nil {
			log.Warnf("Qemu %s is in pending state, %s: %v != %v", qemu.Metadata.Name, v.Key, v.Value, v.Pending)
			isPending = true
		}
	}

	if isPending {
		qemu.Status.Deploy = v1alpha1.STATUS_DEPLOY_PENDING
		qemu, err = kClient.V1alpha1().Qemu().UpdateStatus(qemu)
		if err != nil {
			qemu.Status.Deploy = v1alpha1.STATUS_DEPLOY_ERROR
			qemu, err = kClient.V1alpha1().Qemu().UpdateStatus(qemu)
			if err != nil {
				return qemu, err
			}
			return qemu, err
		}

		return qemu, nil
	}

	currentConfig, err := pClient.Cluster(qemu.Status.Cluster).Node(qemu.Status.Node).Qemu().GetConfig(qemu.Status.VmId)
	if err != nil {
		qemu.Status.Deploy = v1alpha1.STATUS_DEPLOY_ERROR
		qemu, err = kClient.V1alpha1().Qemu().UpdateStatus(qemu)
		if err != nil {
			return qemu, err
		}
		return qemu, err
	}

	designConfig, err := buildQemuConfig(pClient, qemu)
	if err != nil {
		qemu.Status.Deploy = v1alpha1.STATUS_DEPLOY_ERROR
		qemu, err = kClient.V1alpha1().Qemu().UpdateStatus(qemu)
		if err != nil {
			return qemu, err
		}
		return qemu, err
	}

	var outOfSync bool
	for k, v := range designConfig {
		if k == "node" || k == "vmid" {
			continue
		}
		if fmt.Sprint(currentConfig[k]) != fmt.Sprint(v) {
			log.Warnf("Qemu %s is out of sync, %s: %v != %v", qemu.Metadata.Name, k, currentConfig[k], v)
			outOfSync = true
		}
	}

	var syncFail bool
	if outOfSync {
		err = setQemuConfig(kClient, pClient, qemu)
		if err != nil {
			qemu.Status.Deploy = v1alpha1.STATUS_DEPLOY_ERROR
			qemu, err = kClient.V1alpha1().Qemu().UpdateStatus(qemu)
			if err != nil {
				return qemu, err
			}
			syncFail = true
		}
	}

	if syncFail {
		qemu.Status.Deploy = v1alpha1.STATUS_DEPLOY_NOT_SYNCED
	} else {
		qemu.Status.Deploy = v1alpha1.STATUS_DEPLOY_DEPLOYED
	}

	qemu, err = kClient.V1alpha1().Qemu().UpdateStatus(qemu)
	if err != nil {
		qemu.Status.Deploy = v1alpha1.STATUS_DEPLOY_ERROR
		qemu, err = kClient.V1alpha1().Qemu().UpdateStatus(qemu)
		if err != nil {
			return qemu, err
		}
		return qemu, err
	}

	return qemu, nil
}




func syncQemuDisksStatus(kClient *kuberentes.Client, pClient *proxmox.Client, qemu v1alpha1.Qemu) (v1alpha1.Qemu, error) {
	if !isQemuPlaced(qemu) {
		return qemu, nil
	}

	qemuConfig, err := pClient.Cluster(qemu.Status.Cluster).Node(qemu.Status.Node).Qemu().GetConfig(qemu.Status.VmId)
	if err != nil {
		return qemu, err
	}

	rDiskSize := regexp.MustCompile(`^.+size=(.+),?$`)
	for _, disk := range qemu.Spec.Disk {
		var designStorageConfig proxmox.StorageConfig
		designStorageConfig, err = buildStorageConfig(pClient, qemu)
		if err != nil {
			return qemu, fmt.Errorf("cannot build storage config: %s", err)
		}

		for k, v := range qemuConfig {
			if strings.Contains(fmt.Sprint(v), designStorageConfig.Filename) {
				currentSize := rDiskSize.FindStringSubmatch(fmt.Sprint(v))
				if len(currentSize) != 2 {
					return qemu, fmt.Errorf("cannot extract disk num: %s", disk.Name)
				}
				if designStorageConfig.Size != currentSize[1] {
					err = pClient.Cluster(qemu.Status.Cluster).Node(qemu.Status.Node).Qemu().Resize(qemu.Status.VmId, k, designStorageConfig.Size)
					if err != nil {
						return qemu, fmt.Errorf("cannot resize qemu disk: %s", err)
					}
				}
			}
		}
	}

	return qemu, nil
}

func setQemuConfig(kClient *kuberentes.Client, pClient *proxmox.Client, qemu v1alpha1.Qemu) error {
	qemuConfig, err := buildQemuConfig(pClient, qemu)
	if err != nil {
		return err
	}

	err = pClient.Cluster(qemu.Status.Cluster).Node(qemu.Status.Node).Qemu().SetConfig(qemuConfig)
	if err != nil {
		return err
	}

	return nil
}


func isQemuPlaced(qemu v1alpha1.Qemu) bool {
	if qemu.Status.Cluster != "" && qemu.Status.Node != "" && qemu.Status.VmId != 0 {
		return true
	}

	return false
}
